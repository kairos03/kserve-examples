apiVersion: serving.kserve.io/v1beta1
kind: InferenceService
metadata:
  name: onnx-cifar10
spec:
  transformer:
    containers:
      - image: kairos9603/kserve-titon-transfomer:latest
        imagePullPolicy: Always
        name: kserve-container
        command: ["python", "-m", "image_transformer"]
        args:
          - --model_name
          - cifar10
          - --protocol
          - v2
        resources:
          limits:
            cpu: 8
            memory: 10Gi
          requests:
            cpu: 0.5
            memory: 0.5Gi
  predictor:
    nodeSelector:
      nvidia.com/gpu.product: "NVIDIA-GeForce-GTX-1080-Ti"
    triton:
      storageUri: "pvc://onnx-cifar10-pvc"
      runtimeVersion: 22.04-py3
      env:
        - name: OMP_NUM_THREADS
          value: "1"
      # args:
      #   - tritonserver
      #   - --model-store=/mnt/models
      # - --grpc-port=9000
      # - --http-port=8080
      # - --allow-grpc=true
      # - --allow-http=true
      # - -â€“log-verbose=3 --log-info=1 --log-warning=1 --log-error=1
      resources:
        limits:
          cpu: 8
          memory: 10Gi
        requests:
          cpu: 0.2
          memory: 0.5Gi
